{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# LangGraph Demo\n",
    "\n",
    "LangGraph builds stateful, graph-based AI workflows where state flows through nodes connected by edges.\n",
    "\n",
    "## Required Environment Variables\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langgraph langchain-openai python-dotenv\n",
    "\n",
    "import os\n",
    "from typing import List, Literal, TypedDict, Annotated\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-concept",
   "metadata": {},
   "source": [
    "## State Management\n",
    "\n",
    "State is the cornerstone of LangGraph - persistent, shared, and evolves through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketState(TypedDict):\n",
    "    \"\"\"State schema for customer support ticket routing\"\"\"\n",
    "    ticket_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    customer_tier: str\n",
    "    category: str\n",
    "    priority: str\n",
    "    assigned_agent: str\n",
    "    processing_steps: Annotated[List[str], lambda x, y: x + y]  # List reducer\n",
    "    status: str\n",
    "    resolution: str\n",
    "\n",
    "# Example state\n",
    "sample_state = TicketState(\n",
    "    ticket_id=\"TKT-12345\",\n",
    "    title=\"Login Issues with Mobile App\",\n",
    "    description=\"Customer unable to log in using mobile app\",\n",
    "    customer_tier=\"Premium\",\n",
    "    category=\"\",\n",
    "    priority=\"\",\n",
    "    assigned_agent=\"\",\n",
    "    processing_steps=[],\n",
    "    status=\"New\",\n",
    "    resolution=\"\"\n",
    ")\n",
    "\n",
    "print(sample_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nodes-concept",
   "metadata": {},
   "source": [
    "## Nodes & Edges\n",
    "\n",
    "Nodes process state, edges control flow. Each node receives the current state and returns updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nodes-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ticket(state: TicketState) -> dict:\n",
    "    \"\"\"Classify ticket into category using LLM\"\"\"\n",
    "    print(f\"Classifying: {state['ticket_id']}\")\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a ticket classification expert.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            Classify this ticket into: Technical, Billing, or General.\n",
    "            Title: {state['title']}\n",
    "            Description: {state['description']}\n",
    "            Respond with ONLY the category name.\n",
    "        \"\"\")\n",
    "    ])\n",
    "\n",
    "    category = response.content.strip()\n",
    "    if category not in [\"Technical\", \"Billing\", \"General\"]:\n",
    "        category = \"General\"\n",
    "\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"processing_steps\": [f\"Classified as {category}\"],\n",
    "        \"status\": \"Classified\"\n",
    "    }\n",
    "\n",
    "def route_priority(state: TicketState) -> dict:\n",
    "    \"\"\"Determine ticket priority using LLM\"\"\"\n",
    "    print(f\"Setting priority: {state['ticket_id']}\")\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a support ticket priority expert.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            Determine priority (High, Medium, or Low) for:\n",
    "            Category: {state['category']}\n",
    "            Customer Tier: {state['customer_tier']}\n",
    "            Title: {state['title']}\n",
    "            Respond with ONLY the priority level.\n",
    "        \"\"\")\n",
    "    ])\n",
    "\n",
    "    priority = response.content.strip()\n",
    "    if priority not in [\"High\", \"Medium\", \"Low\"]:\n",
    "        priority = \"Medium\"\n",
    "\n",
    "    return {\n",
    "        \"priority\": priority,\n",
    "        \"processing_steps\": [f\"Priority set to {priority}\"],\n",
    "        \"status\": \"Prioritized\"\n",
    "    }\n",
    "\n",
    "def match_agent(state: TicketState) -> dict:\n",
    "    \"\"\"Assign appropriate agent using LLM\"\"\"\n",
    "    print(f\"Assigning agent: {state['ticket_id']}\")\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You match tickets to agents.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            Match agent for: {state['category']} issue, {state['priority']} priority\n",
    "            Options: Senior Tech Support, Tech Support, Billing Specialist, General Support\n",
    "            Respond with ONLY the agent role name.\n",
    "        \"\"\")\n",
    "    ])\n",
    "\n",
    "    agent = response.content.strip()\n",
    "    valid_agents = [\"Senior Tech Support\", \"Tech Support\", \"Billing Specialist\", \"General Support\"]\n",
    "    if agent not in valid_agents:\n",
    "        agent = \"General Support\"\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": agent,\n",
    "        \"processing_steps\": [f\"Assigned to {agent}\"],\n",
    "        \"status\": \"Assigned\"\n",
    "    }\n",
    "\n",
    "def generate_resolution(state: TicketState) -> dict:\n",
    "    \"\"\"Generate resolution using LLM\"\"\"\n",
    "    print(f\"Resolving: {state['ticket_id']}\")\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a support resolution specialist.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            Generate a brief resolution (max 30 words) for:\n",
    "            Ticket: {state['title']}\n",
    "            Category: {state['category']}\n",
    "            Agent: {state['assigned_agent']}\n",
    "        \"\"\")\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"resolution\": response.content.strip(),\n",
    "        \"processing_steps\": [\"Resolution generated\"],\n",
    "        \"status\": \"Resolved\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-graph-concept",
   "metadata": {},
   "source": [
    "## Build & Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(TicketState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify\", classify_ticket)\n",
    "workflow.add_node(\"prioritize\", route_priority)\n",
    "workflow.add_node(\"assign\", match_agent)\n",
    "workflow.add_node(\"resolve\", generate_resolution)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"classify\")\n",
    "workflow.add_edge(\"classify\", \"prioritize\")\n",
    "workflow.add_edge(\"prioritize\", \"assign\")\n",
    "workflow.add_edge(\"assign\", \"resolve\")\n",
    "workflow.add_edge(\"resolve\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Visualize\n",
    "mermaid_text = app.get_graph().draw_mermaid()\n",
    "lines = mermaid_text.split('\\n')\n",
    "start_idx = next(i for i, line in enumerate(lines) if 'graph' in line.lower())\n",
    "display(Markdown(f\"```mermaid\\n{'\\n'.join(lines[start_idx:])}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute-concept",
   "metadata": {},
   "source": [
    "## Execute Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-workflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ticket = {\n",
    "    \"ticket_id\": \"TKT-001\",\n",
    "    \"title\": \"Mobile app crashes on startup\",\n",
    "    \"description\": \"The iOS app crashes immediately after opening\",\n",
    "    \"customer_tier\": \"Premium\",\n",
    "    \"category\": \"\",\n",
    "    \"priority\": \"\",\n",
    "    \"assigned_agent\": \"\",\n",
    "    \"processing_steps\": [],\n",
    "    \"status\": \"New\",\n",
    "    \"resolution\": \"\"\n",
    "}\n",
    "\n",
    "result = app.invoke(test_ticket)\n",
    "\n",
    "print(f\"\\nResult:\")\n",
    "print(f\"  Category: {result['category']}\")\n",
    "print(f\"  Priority: {result['priority']}\")\n",
    "print(f\"  Agent: {result['assigned_agent']}\")\n",
    "print(f\"  Resolution: {result['resolution']}\")\n",
    "print(f\"  Steps: {result['processing_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-concept",
   "metadata": {},
   "source": [
    "## Conditional Edges\n",
    "\n",
    "Conditional edges enable dynamic routing based on state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTicketState(TypedDict):\n",
    "    \"\"\"State with conditional routing fields\"\"\"\n",
    "    ticket_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    customer_tier: str\n",
    "    category: str\n",
    "    priority: str\n",
    "    assigned_agent: str\n",
    "    needs_escalation: bool\n",
    "    processing_steps: Annotated[List[str], lambda x, y: x + y]\n",
    "    status: str\n",
    "    resolution: str\n",
    "\n",
    "llm_senior = ChatOpenAI(model=\"gpt-4o\")  # More capable for escalated tickets\n",
    "\n",
    "def escalation_check(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Determine if ticket needs escalation\"\"\"\n",
    "    print(f\"Checking escalation: {state['ticket_id']}\")\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You determine if tickets need senior attention.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            Should this be escalated? (YES/NO)\n",
    "            Priority: {state['priority']}, Customer: {state['customer_tier']}\n",
    "            Title: {state['title']}\n",
    "            Escalate if: High priority, Premium+Technical, or mentions critical/urgent/outage.\n",
    "        \"\"\")\n",
    "    ])\n",
    "\n",
    "    needs_escalation = response.content.strip().upper() == \"YES\"\n",
    "    return {\n",
    "        \"needs_escalation\": needs_escalation,\n",
    "        \"processing_steps\": [f\"Escalation: {'Yes' if needs_escalation else 'No'}\"],\n",
    "        \"status\": \"Escalation Checked\"\n",
    "    }\n",
    "\n",
    "def senior_agent_resolution(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Senior agent handles escalated tickets with GPT-4o\"\"\"\n",
    "    print(f\"Senior agent handling: {state['ticket_id']}\")\n",
    "\n",
    "    response = llm_senior.invoke([\n",
    "        SystemMessage(content=\"You are a senior technical expert.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            Provide detailed resolution (50 words) for escalated ticket:\n",
    "            Title: {state['title']}\n",
    "            Category: {state['category']}\n",
    "        \"\"\")\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": \"Senior Agent (GPT-4o)\",\n",
    "        \"resolution\": response.content.strip(),\n",
    "        \"processing_steps\": [\"Resolved by Senior Agent\"],\n",
    "        \"status\": \"Resolved\"\n",
    "    }\n",
    "\n",
    "def normal_agent_resolution(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Normal agent handles standard tickets with GPT-4o-mini\"\"\"\n",
    "    print(f\"Normal agent handling: {state['ticket_id']}\")\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a support agent.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            Provide brief resolution (25 words) for:\n",
    "            Title: {state['title']}\n",
    "            Category: {state['category']}\n",
    "        \"\"\")\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": \"Normal Agent (GPT-4o-mini)\",\n",
    "        \"resolution\": response.content.strip(),\n",
    "        \"processing_steps\": [\"Resolved by Normal Agent\"],\n",
    "        \"status\": \"Resolved\"\n",
    "    }\n",
    "\n",
    "def route_after_escalation(state: EnhancedTicketState) -> Literal[\"senior_agent\", \"normal_agent\"]:\n",
    "    \"\"\"Route based on escalation decision\"\"\"\n",
    "    return \"senior_agent\" if state['needs_escalation'] else \"normal_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_workflow = StateGraph(EnhancedTicketState)\n",
    "\n",
    "conditional_workflow.add_node(\"classify\", classify_ticket)\n",
    "conditional_workflow.add_node(\"prioritize\", route_priority)\n",
    "conditional_workflow.add_node(\"escalation_check\", escalation_check)\n",
    "conditional_workflow.add_node(\"senior_agent\", senior_agent_resolution)\n",
    "conditional_workflow.add_node(\"normal_agent\", normal_agent_resolution)\n",
    "\n",
    "conditional_workflow.add_edge(START, \"classify\")\n",
    "conditional_workflow.add_edge(\"classify\", \"prioritize\")\n",
    "conditional_workflow.add_edge(\"prioritize\", \"escalation_check\")\n",
    "conditional_workflow.add_conditional_edges(\n",
    "    \"escalation_check\",\n",
    "    route_after_escalation,\n",
    "    {\"senior_agent\": \"senior_agent\", \"normal_agent\": \"normal_agent\"}\n",
    ")\n",
    "conditional_workflow.add_edge(\"senior_agent\", END)\n",
    "conditional_workflow.add_edge(\"normal_agent\", END)\n",
    "\n",
    "conditional_app = conditional_workflow.compile()\n",
    "\n",
    "# Visualize\n",
    "mermaid_text = conditional_app.get_graph().draw_mermaid()\n",
    "lines = mermaid_text.split('\\n')\n",
    "start_idx = next(i for i, line in enumerate(lines) if 'graph' in line.lower())\n",
    "display(Markdown(f\"```mermaid\\n{'\\n'.join(lines[start_idx:])}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-scenarios",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Critical issue -> Senior Agent\n",
    "critical_ticket = {\n",
    "    \"ticket_id\": \"TKT-CRITICAL\",\n",
    "    \"title\": \"URGENT: Production database down\",\n",
    "    \"description\": \"Critical system failure, complete outage\",\n",
    "    \"customer_tier\": \"Premium\",\n",
    "    \"category\": \"\", \"priority\": \"\", \"assigned_agent\": \"\",\n",
    "    \"needs_escalation\": False, \"processing_steps\": [],\n",
    "    \"status\": \"New\", \"resolution\": \"\"\n",
    "}\n",
    "\n",
    "print(\"Scenario 1: Critical Production Outage\")\n",
    "result1 = conditional_app.invoke(critical_ticket)\n",
    "print(f\"  Agent: {result1['assigned_agent']}\")\n",
    "print(f\"  Resolution: {result1['resolution'][:100]}...\\n\")\n",
    "\n",
    "# Scenario 2: Simple question -> Normal Agent\n",
    "simple_ticket = {\n",
    "    \"ticket_id\": \"TKT-SIMPLE\",\n",
    "    \"title\": \"How do I change my email address?\",\n",
    "    \"description\": \"I want to update my account email\",\n",
    "    \"customer_tier\": \"Standard\",\n",
    "    \"category\": \"\", \"priority\": \"\", \"assigned_agent\": \"\",\n",
    "    \"needs_escalation\": False, \"processing_steps\": [],\n",
    "    \"status\": \"New\", \"resolution\": \"\"\n",
    "}\n",
    "\n",
    "print(\"Scenario 2: Simple Account Question\")\n",
    "result2 = conditional_app.invoke(simple_ticket)\n",
    "print(f\"  Agent: {result2['assigned_agent']}\")\n",
    "print(f\"  Resolution: {result2['resolution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "LangGraph provides graph-based workflow orchestration with:\n",
    "- **State**: Persistent, shared data that evolves through the workflow\n",
    "- **Nodes**: Functions that process state and return updates\n",
    "- **Edges**: Fixed or conditional connections between nodes\n",
    "- **Conditional routing**: Dynamic paths based on state values\n",
    "\n",
    "### Resources\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [GitHub: langchain-ai/langgraph](https://github.com/langchain-ai/langgraph)\n",
    "- [LangChain Academy LangGraph Course](https://academy.langchain.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
